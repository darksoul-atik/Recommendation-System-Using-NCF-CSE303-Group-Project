# -*- coding: utf-8 -*-
"""Copy of CSE303_Spotify_Recommended_System_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oi0v5zKjf-6CZ60cfwvtvQCN77fGJL8v
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install dash
!pip install pyngrok

from dash import Dash, html, dcc, Input, Output
from dash.dependencies import Input, Output
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score

import plotly.express as px
from pyngrok import ngrok

# Load dataset
data = pd.read_csv('/content/drive/MyDrive/colab-datasets/Spotify/data.csv')
genre_data = pd.read_csv('/content/drive/MyDrive/colab-datasets/Spotify/data_by_genres.csv')
year_data = pd.read_csv('/content/drive/MyDrive/colab-datasets/Spotify/data_by_year.csv')
artist_data = pd.read_csv('/content/drive/MyDrive/colab-datasets/Spotify/data_by_artist.csv')


print(data.head())

print(data)

print(data.isnull().sum())

unique_artists = data.drop_duplicates(subset='artists')
unique_artists.head()

data.shape
unique_artists.shape

data.describe().transpose()

from sklearn.preprocessing import StandardScaler

# Define the features to normalize
features_to_normalize = ['tempo', 'loudness', 'energy']

# Initialize the StandardScaler
scaler = StandardScaler()

# Normalize the selected features using Z-score normalization
data[features_to_normalize] = scaler.fit_transform(data[features_to_normalize])

# Print the summary statistics of the normalized features
print(data[features_to_normalize].describe())

data.update(features_to_normalize)

data['release_date'] = pd.to_datetime(data['release_date'], errors='coerce')
data['year'] = data['release_date'].dt.year

categorical_data = data.select_dtypes(include = 'object')

categorical_data.info()

# Drop irrelevant columns
data.drop(columns=['id', 'release_date', 'mode'], inplace=True)

numerical_features = ['danceability', 'energy', 'loudness', 'tempo', 'valence']
print(data[numerical_features].max())

# Compute the correlation matrix for the normalized features
correlation_matrix = data[numerical_features].corr()

# Print the correlation matrix
print(correlation_matrix)

import plotly.express as px

# Create an interactive 3D scatter plot
fig = px.scatter_3d(data,
                     x='tempo',
                     y='loudness',
                     z='energy',
                     color='energy',
                     opacity=0.8)

# Show the plot
fig.show()

# print(data[features_to_normalize].describe())

plt.figure(figsize=(12, 4))
for i, feature in enumerate(features_to_normalize):
    plt.subplot(1, len(features_to_normalize), i + 1)
    sns.histplot(data[feature], kde=True, bins=30)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 4))
sns.boxplot(data=data[features_to_normalize])
plt.title("Box Plot of Normalized Features")
plt.show()

sns.pairplot(data[features_to_normalize])
plt.show()

plt.figure(figsize=(6, 5))
sns.heatmap(data[features_to_normalize].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

print(data[features_to_normalize])

data.update(features_to_normalize)
print(data)

high_danceability = unique_artists.query("0.5 <= danceability <= 1")
# print(high_danceability)

sns.histplot(high_danceability['danceability'], kde=True, bins=30, color='blue')
plt.title('Distribution of High Danceability Songs')
plt.xlabel('Danceability')
plt.ylabel('Count')
plt.show()

#  data.drop(['cluster'], axis=1)

# clusters.shape

data['is_popular'] = (data['popularity'] > 60).astype(int)
y = data['is_popular']
# print(data)

sns.histplot(data=data, x="popularity", kde=True, color="blue", bins=10)
plt.title("Popularity Distribution of High-Danceability Songs")
plt.show()

dance_corr_matrix = high_danceability[["danceability", "energy", "tempo", "popularity", "loudness"]].corr()
sns.heatmap(dance_corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation for High-Danceability Songs")
plt.show()

Q1 = data['duration_ms'].quantile(0.25)
Q3 = data['duration_ms'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['duration_ms'] < lower_bound) | (data['duration_ms'] > upper_bound)]
print("Number of outliers:", len(outliers))

sns.boxplot(x=data['duration_ms'])
plt.title("Boxplot of Duration (ms)")
plt.show()

avg_duration_by_artist = data.groupby('artists')['duration_ms'].mean().reset_index()
top_10_artists = avg_duration_by_artist.nlargest(10, 'duration_ms')

sns.barplot(data=top_10_artists, x='artists', y='duration_ms', palette='Blues_d')
plt.title("Average Duration (ms) by Artist (Top 10 Artists)")
plt.xlabel("Artist")
plt.ylabel("Average Duration (ms)")
plt.xticks(rotation=60, ha="right")
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

avg_duration_by_year = data.groupby('year')['duration_ms'].mean().reset_index()

sns.lineplot(data=avg_duration_by_year, x='year', y='duration_ms', color='blue')
plt.title("Average Duration (ms) by Year")
plt.xlabel("Year")
plt.ylabel("Average Duration (ms)")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

numeric_cols = data[features_to_normalize].select_dtypes(include=['float64', 'int64']).columns

for col in numeric_cols[:1]:
    plt.figure(figsize=(10, 4))
    sns.boxplot(x=data[col])
    plt.title(f"Boxplot for {col}")
    plt.show()

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=5, random_state=42)
features = data[features_to_normalize]
data['cluster'] = kmeans.fit_predict(features)

plt.figure(figsize=(10, 6))
sns.scatterplot(
    x=data['tempo'],
    y=data['energy'],
    hue=data['cluster'],
    palette='viridis',
    s=50
)
plt.title('Clusters of Songs')
plt.xlabel('Tempo')
plt.ylabel('Energy')
plt.legend(title='Cluster')
plt.show()

sns.pairplot(data[['danceability', 'energy', 'tempo', 'cluster']], hue='cluster', palette='viridis')
plt.show()

import plotly.express as px

# Create a scatter plot with 'energy' on the x-axis, 'popularity' on the y-axis,
# and color the points based on 'duration_ms'.
fig = px.scatter(
    genre_data,
    x='energy',
    y='popularity',
    color='duration_ms',  # Color based on duration
    trendline='ols',      # Add a trendline (requires statsmodels)
    title='Popularity vs Energy (Colored by Duration)',
    color_continuous_scale='viridis',  # Correct argument
)

# Show the plot
fig.show()

import matplotlib.pyplot as plt
import numpy as np

# Normalize the 'duration_ms' column for better colormap scaling
norm = plt.Normalize(genre_data['duration_ms'].min(), genre_data['duration_ms'].max())
cmap = plt.cm.viridis

plt.figure(figsize=(10, 6))
scatter = plt.scatter(
    genre_data['energy'],
    genre_data['popularity'],
    c=genre_data['duration_ms'],
    cmap=cmap,
    s=50,
    alpha=0.7
)

# Add a colorbar to indicate the mapping of colors to values
cbar = plt.colorbar(scatter)
cbar.set_label('Duration (ms)', fontsize=12)

# Add labels and title
plt.xlabel('Energy', fontsize=12)
plt.ylabel('Popularity', fontsize=12)
plt.title('Energy vs Popularity (Colored by Duration)', fontsize=14)
plt.grid(alpha=0.3)

# Show the plot
plt.show()

import pandas as pd
import plotly.express as px

# Normalize duration_ms to minutes for better visualization
genre_data['duration_min'] = genre_data['duration_ms'] / 60000

# Select relevant columns for the plot
features = ['genres', 'acousticness', 'danceability', 'duration_min', 'energy',
            'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence', 'popularity']

# Filter the dataset to include only the selected features
data_filtered = genre_data[features]

# Create the parallel coordinates plot
fig = px.parallel_coordinates(
    data_filtered,
    color='popularity',  # Color by popularity
    dimensions=['acousticness', 'danceability', 'duration_min', 'energy',
                'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence'],
    title='Audio Features vs Popularity Across Genres',
    color_continuous_scale=px.colors.sequential.Viridis,
    labels={
        'acousticness': 'Acousticness',
        'danceability': 'Danceability',
        'duration_min': 'Duration (min)',
        'energy': 'Energy',
        'instrumentalness': 'Instrumentalness',
        'liveness': 'Liveness',
        'loudness': 'Loudness',
        'speechiness': 'Speechiness',
        'tempo': 'Tempo',
        'valence': 'Valence'
    }
)

# Show the plot
fig.show()

import pandas as pd
import numpy as np
import plotly.graph_objects as go

# Load the dataset
genre_data = pd.read_csv('/content/drive/MyDrive/colab-datasets/Spotify/data_by_genres.csv')

# Select top genres by average popularity
top_genres = genre_data.groupby('genres')['popularity'].mean().sort_values(ascending=False).head(10).index

# Filter data for top genres
top_data = genre_data[genre_data['genres'].isin(top_genres)]

# Calculate average values for each feature
avg_features = top_data.groupby('genres')[['acousticness', 'danceability', 'energy', 'loudness', 'speechiness', 'tempo', 'valence']].mean()

# Convert to a list of lists for plotting
categories = ['acousticness', 'danceability', 'energy', 'loudness', 'speechiness', 'tempo', 'valence']
values = avg_features.values.tolist()

# Create radar chart
fig = go.Figure()

for i, genre in enumerate(avg_features.index):  # Changed genre_data to genre for clarity
    fig.add_trace(go.Scatterpolar(
        r=values[i] + [values[i][0]],  # Close the loop
        theta=categories + [categories[0]],  # Close the loop
        fill='toself',
        name=genre  # Use genre as the name
    ))

# Update layout
fig.update_layout(
    polar=dict(
        radialaxis=dict(visible=True, range=[0, 1])
    ),
    title='Comparison of Top Genres by Audio Features',
    title_x=0.5,
    showlegend=True
)

# Show the plot
fig.show()

plt.figure(figsize=(12, 6))

# Loudness trend
ax1 = sns.lineplot(x='year', y='loudness', data=year_data, label='Loudness', color='red')

# Acousticness trend (inverse)
ax2 = ax1.twinx()
sns.lineplot(x='year', y=-year_data['acousticness'], data=year_data, label='Inverse Acousticness', ax=ax2, color='blue')


# Add annotations for key years
key_years = [1960, 1980, 2000, 2020]
for year in key_years:
    loudness_val = year_data.loc[year_data['year'] == year, 'loudness'].values[0]
    acousticness_val = -year_data.loc[year_data['year'] == year, 'acousticness'].values[0]
    ax1.annotate(f'{year}', (year, loudness_val), textcoords="offset points", xytext=(0,10), ha='center', fontsize=10)
    ax2.annotate(f'{year}', (year, acousticness_val), textcoords="offset points", xytext=(0,-15), ha='center', fontsize=10)


plt.title('The Loudness War: Louder Music, Less Acoustic')
ax1.set_xlabel('Year')
ax1.set_ylabel('Loudness (dB)', color='red')
ax2.set_ylabel('Inverse Acousticness', color='blue')
plt.legend(loc='upper left')
plt.show()

# Create a new column for decades
year_data['decade'] = (year_data['year'] // 10) * 10

# Calculate proportions
year_data['speechiness_prop'] = year_data['speechiness'] / (year_data['speechiness'] + year_data['instrumentalness'])
year_data['instrumentalness_prop'] = year_data['instrumentalness'] / (year_data['speechiness'] + year_data['instrumentalness'])

# Group by decade and calculate mean proportions
proportions_by_decade = year_data.groupby('decade')[['speechiness_prop', 'instrumentalness_prop']].mean().reset_index()

# Stacked bar chart
plt.figure(figsize=(12, 6))
sns.barplot(x='decade', y='speechiness_prop', data=proportions_by_decade, color='orange', label='Speechiness')
sns.barplot(x='decade', y='instrumentalness_prop', data=proportions_by_decade, bottom=proportions_by_decade['speechiness_prop'], color='purple', label='Instrumentalness')

plt.title('Speechiness vs. Instrumentalness by Decade')
plt.xlabel('Decade')
plt.ylabel('Proportion')
plt.xticks(rotation=45)
plt.legend()
plt.show()

"""## Applying PCA"""

data.columns

from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Drop irrelevant columns
data.drop(columns=['artists','name','year'], inplace=True)

# Split dataset
X = data.drop(columns=['popularity'])
y = data['popularity']
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# # print(X_train.dtypes)  # Check column data types
# for col in X_train.columns:
#     if X_train[col].dtype == 'object':  # If column contains strings
#         print(f"Non-numeric column: {col}")
#         print(X_train[col].unique())  # Show unique values in the column



# print(X_train.isnull().sum())  # Check for missing values in each column
# print(X_val.isnull().sum())
# print(X_test.isnull().sum())

from sklearn.decomposition import PCA
from sklearn.preprocessing import PolynomialFeatures

# Apply PCA
pca = PCA(n_components=5)
X_train_pca = pca.fit_transform(X_train)
X_val_pca = pca.transform(X_val)
X_test_pca = pca.transform(X_test)

# Create interaction terms
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_val_poly = poly.transform(X_val)
X_test_poly = poly.transform(X_test)

"""## OLS MODEL"""

import statsmodels.api as sm

from sklearn.preprocessing import PolynomialFeatures


X_OLS = data[['danceability']]  # Example with two features
Y_OLS = data['popularity']    # Target variable# Prepare the data for statsmodels
X_OLS = sm.add_constant(X_OLS)  # Adds a constant term (intercept) to the predictor

# poly = PolynomialFeatures(degree=2, include_bias=False)
# X_poly = poly.fit_transform(data[['danceability', 'energy']])
# X_OLS = sm.add_constant(X_poly)

# Create the OLS model
model = sm.OLS(Y_OLS, X_OLS)
results = model.fit()

# Print the summary of the regression
print(results.summary())

# Predicted values
predictions = results.predict(X_OLS)

# residuals = Y_OLS - predictions
# plt.scatter(predictions, residuals)
# plt.axhline(0, color='red', linestyle='--')
# plt.xlabel("Predicted Values")
# plt.ylabel("Residuals")
# plt.title("Residual Plot")
# plt.show()

# Visualize the results
plt.scatter(data['danceability'], Y_OLS, label="Data")
plt.plot(data['danceability'], predictions, color="red", label="Fitted Line")
plt.xlabel("Danceability")
plt.ylabel("Popularity")
plt.legend()
plt.title("Linear Regression using OLS")
plt.show()

"""## Logistic Regression"""

print(data.columns)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                             roc_curve, auc)

# Define binary target based on 'popularity'
data['like'] = (data['popularity'] > 50).astype(int)

# For example, if 'popularity' and 'like' should be excluded:
X = data.drop(['popularity', 'like'], axis=1)
y = data['like']

# Split Data into Training, Validation, and Test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)


# Visualize Dataset
# (Assuming the first two columns of X are representative for a 2D scatter plot)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y, palette="coolwarm", alpha=0.7)
plt.title("Dataset Visualization")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# Train Logistic Regression Model
logreg_model = LogisticRegression()
logreg_model.fit(X_train, y_train)

# Model Predictions on Test Data
y_pred = logreg_model.predict(X_test)

# Model Evaluation: Accuracy and Classification Report
accuracy = accuracy_score(y_test, y_pred)
print(f"Logistic Regression Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve and AUC
y_prob = logreg_model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()

# Model Coefficients
print("Model Coefficients:")
print(f"Intercept: {logreg_model.intercept_[0]:.2f}")
for i, coef in enumerate(logreg_model.coef_[0]):
    print(f"Coefficient for feature {i + 1}: {coef:.2f}")

"""## MLE"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

# Define the target variable
data['like'] = (data['popularity'] > 50).astype(int)

# Prepare the data: drop 'popularity' and 'like' from features
X = data.drop(['popularity', 'like'], axis=1)
X = sm.add_constant(X)  # Add intercept term
y = data['like']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit the Logit model (MLE-based logistic regression)
logit_model = sm.Logit(y_train, X_train)
results = logit_model.fit()

# Print the summary
print(results.summary())

# Predict probabilities on the test set
y_prob = results.predict(X_test)

# Convert probabilities to binary predictions using 0.5 threshold
y_pred = (y_prob >= 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix plot
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ----------------------------
# ROC Curve
# ----------------------------
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})", color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], linestyle="--", color="gray", lw=2)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.show()

# ----------------------------
# Logistic Plot for one feature
# ----------------------------
# Choose a predictor to visualize. We use the first non-constant column.
feature_to_plot = X.columns[1]  # assuming column 0 is the constant

# Create a range of values for the chosen feature
x_min = X_test[feature_to_plot].min()
x_max = X_test[feature_to_plot].max()
x_range = np.linspace(x_min, x_max, 100)

# Create a DataFrame that holds the mean values for all features
# and varies only the chosen feature
plot_df = X_test.copy().iloc[:100].mean().to_frame().T  # start with mean values
plot_df = plot_df.loc[plot_df.index.repeat(100)].reset_index(drop=True)
plot_df[feature_to_plot] = x_range

# Predict probabilities using the fitted model for the grid
predicted_probs = results.predict(plot_df)

plt.figure(figsize=(8, 6))
plt.plot(x_range, predicted_probs, color="green", lw=2, label="Logistic Curve")
plt.scatter(X_test[feature_to_plot], y_test, alpha=0.3, label="Data", color="blue")
plt.xlabel(feature_to_plot)
plt.ylabel("Predicted Probability of like=1")
plt.title(f"Logistic Plot for Feature: {feature_to_plot}")
plt.legend()
plt.show()

"""## Elasticnet Regularization"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    precision_recall_curve,
    average_precision_score,
)
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE  # For handling class imbalance
from sklearn.pipeline import Pipeline

# Define the target variable
data['like'] = (data['popularity'] > 50).astype(int)

# Prepare the data: drop 'popularity' and 'like' from features
X = data.drop(['popularity', 'like'], axis=1)
y = data['like']

# Split the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Handle class imbalance using SMOTE
# smote = SMOTE(random_state=42)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# ----------------------------
# Cross-Validation for Logistic Regression (MLE) with PCA
# ----------------------------

logreg_pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Step 1: Scale the data
    ('pca', PCA()),               # Step 2: Apply PCA
    ('logreg', LogisticRegression(max_iter=10000, class_weight='balanced', random_state=42))  # Step 3: Logistic Regression
])

logreg_param_grid = {
    'pca__n_components': [5, 10, 15, 20],  # Try more PCA components
    'logreg__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],  # Wider range for regularization strength
    'logreg__penalty': ['l1', 'l2', 'elasticnet'],  # Include all penalties
    'logreg__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],  # More granularity for ElasticNet mixing
    'logreg__solver': ['saga']  # Supports all penalties
}

logreg_grid_search = GridSearchCV(logreg_pipeline, logreg_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
logreg_grid_search.fit(X_train, y_train)

# Best Logistic Regression model
best_logreg_model = logreg_grid_search.best_estimator_
print(f"Best Logistic Regression Parameters: {logreg_grid_search.best_params_}")

# ----------------------------
# Evaluate the Model
# ----------------------------

# Predict probabilities on the test set
y_prob = best_logreg_model.predict_proba(X_test)[:, 1]

# Convert probabilities to binary predictions using 0.5 threshold
y_pred = (y_prob >= 0.5).astype(int)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nLogistic Regression Evaluation:")
print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix plot
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Logistic Regression Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})", color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], linestyle="--", color="gray", lw=2)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Logistic Regression ROC Curve")
plt.legend(loc="lower right")
plt.show()

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_prob)
average_precision = average_precision_score(y_test, y_prob)
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f"Precision-Recall Curve (AP = {average_precision:.2f})", color='blue', lw=2)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Logistic Regression Precision-Recall Curve")
plt.legend(loc="lower left")
plt.show()

"""## Recommended System"""

X_train.columns

!pip install scikit-surprise

"""## Hybrid Model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split as surprise_train_test_split

data = pd.read_csv('/content/drive/MyDrive/colab-datasets/Spotify/data.csv')


# ----------------------------
# Load and Preprocess Data
# ----------------------------

# Define the target variable: Convert 'popularity' into a binary label
data['like'] = (data['popularity'] > 50).astype(int)

# Drop non-numeric and non-useful features
drop_columns = ['id', 'name', 'artists', 'release_date', 'popularity']
X = data.drop(columns=drop_columns + ['like'])
y = data['like']

# Split into training and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# ----------------------------
# Content-Based Filtering (Logistic Regression)
# ----------------------------
# Define Pipeline
logreg_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=10)),  # Reduce dimensionality
    ('logreg', LogisticRegression(max_iter=10000, class_weight='balanced', random_state=42))
])

# Train Logistic Regression
logreg_pipeline.fit(X_train_resampled, y_train_resampled)

# Predict probabilities for all songs
logreg_preds = logreg_pipeline.predict_proba(X)[:, 1]

# ----------------------------
# Collaborative Filtering (Matrix Factorization - SVD)
# ----------------------------
# Create user-song interaction matrix
user_song_data = data[['id', 'popularity']].copy()
user_song_data['user_id'] = np.random.randint(1, 1001, size=len(user_song_data))  # Fake user IDs
user_song_data['interaction'] = (user_song_data['popularity'] > 50).astype(int)

# Prepare data for Surprise library
reader = Reader(rating_scale=(0, 1))
interaction_data = Dataset.load_from_df(user_song_data[['user_id', 'id', 'interaction']], reader)

# Train-test split
trainset, testset = surprise_train_test_split(interaction_data, test_size=0.3)

# Train SVD model
svd = SVD(n_factors=50, random_state=42)
svd.fit(trainset)

# Generate SVD predictions for all songs
cf_preds = np.array([svd.predict(uid, iid).est for uid, iid in zip(user_song_data['user_id'], user_song_data['id'])])

# ----------------------------
# Similarity-Based Filtering (Nearest Neighbors)
# ----------------------------
# Normalize features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Fit Nearest Neighbors model using brute force algorithm
nn_model = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute')
nn_model.fit(X_scaled)

def get_similar_songs_nn(song_id, data, nn_model, n_neighbors=10):
    """
    Find similar songs using Nearest Neighbors.
    Parameters:
    - song_id: ID of the song to find similar songs for.
    - data: Dataset containing song features.
    - nn_model: Trained Nearest Neighbors model.
    - n_neighbors: Number of similar songs to return.
    Returns:
    - DataFrame of similar songs.
    """
    # Get index of the song
    song_index = data.index[data['id'] == song_id].tolist()[0]

    # Get feature vector of the song
    song_vector = X_scaled[song_index].reshape(1, -1)

    # Find nearest neighbors (excluding itself)
    distances, indices = nn_model.kneighbors(song_vector, n_neighbors=n_neighbors+1)

    # Get similar songs
    similar_songs = data.iloc[indices[0][1:]][['id', 'name', 'artists']]

    return similar_songs

# ----------------------------
# Hybrid Model (Weighted Combination)
# ----------------------------
def recommend_songs_hybrid_nn(user_id, data, svd_model, logreg_model, nn_model, alpha=0.5, beta=0.3, gamma=0.2, top_n=10):
    """
    Recommend songs using a hybrid model with Nearest Neighbors.
    Parameters:
    - user_id: ID of the user for whom recommendations are generated.
    - data: The dataset containing song information.
    - svd_model: Trained SVD model.
    - logreg_model: Trained logistic regression pipeline.
    - nn_model: Trained Nearest Neighbors model.
    - alpha: Weight for content-based filtering.
    - beta: Weight for collaborative filtering.
    - gamma: Weight for similarity-based filtering.
    - top_n: Number of recommendations to return.
    Returns:
    - DataFrame of top N recommended songs.
    """
    # Get songs the user has interacted with
    user_songs = user_song_data[user_song_data['user_id'] == user_id]

    # If user has no history, return top popular songs
    if user_songs.empty:
        print("No user history found, recommending popular songs.")
        return data.sort_values(by="popularity", ascending=False).head(top_n)[['id', 'name', 'artists']]

    # Get content-based predictions (Logistic Regression)
    song_features = data.drop(columns=['id', 'name', 'artists', 'release_date', 'popularity', 'like'])
    logreg_preds = logreg_model.predict_proba(song_features)[:, 1]

    # Get collaborative filtering predictions (SVD)
    cf_preds = np.array([svd_model.predict(user_id, song_id).est for song_id in data['id']])

    # Get similarity-based recommendations
    similar_scores = np.zeros(len(data))

    for song_id in user_songs['id']:
        similar_songs = get_similar_songs_nn(song_id, data, nn_model, n_neighbors=10)
        similar_scores[data.index[data['id'].isin(similar_songs['id'])]] += 1

    similar_scores /= similar_scores.max()  # Normalize scores

    # Normalize all scores
    scaler = MinMaxScaler()
    logreg_preds_norm = scaler.fit_transform(logreg_preds.reshape(-1, 1)).flatten()
    cf_preds_norm = scaler.fit_transform(cf_preds.reshape(-1, 1)).flatten()
    similar_scores_norm = scaler.fit_transform(similar_scores.reshape(-1, 1)).flatten()

    # Compute final hybrid score
    hybrid_scores = alpha * logreg_preds_norm + beta * cf_preds_norm + gamma * similar_scores_norm

    # Debug: Print hybrid scores
    print(f"Hybrid scores for user {user_id}: {hybrid_scores[:10]}")

    # Create a recommendations DataFrame
    recommendations = data[['id', 'name', 'artists']].copy()
    recommendations['hybrid_score'] = hybrid_scores

    # Sort by highest predicted score
    recommendations = recommendations.sort_values(by='hybrid_score', ascending=False)

    return recommendations.head(top_n)

# ----------------------------
# Evaluate Hybrid Model
# ----------------------------
def evaluate_hybrid_model(alpha, beta, gamma, data, svd_model, logreg_model, nn_model, user_id, top_n=10):
    # Generate hybrid recommendations
    recommendations = recommend_songs_hybrid_nn(
        user_id=user_id,
        data=data,
        svd_model=svd_model,
        logreg_model=logreg_model,
        nn_model=nn_model,
        alpha=alpha,
        beta=beta,
        gamma=gamma,
        top_n=top_n
    )

    # Simulate ground truth relevance (e.g., based on popularity or other criteria)
    relevant_tracks = data.sort_values(by="popularity", ascending=False).head(top_n)['id'].values
    recommended_tracks = recommendations['id'].values

    # Debug: Print relevant and recommended tracks
    print(f"Relevant Tracks: {relevant_tracks}")
    print(f"Recommended Tracks: {recommended_tracks}")

    # Calculate precision@K
    precision = len(set(recommended_tracks).intersection(set(relevant_tracks))) / top_n if top_n > 0 else 0
    return precision

# ----------------------------
# Simplified Weight Tuning
# ----------------------------
weights_to_test = [
    {'alpha': 0.5, 'beta': 0.3, 'gamma': 0.2},
    {'alpha': 0.7, 'beta': 0.2, 'gamma': 0.1},
    {'alpha': 0.3, 'beta': 0.5, 'gamma': 0.2}
]

best_precision = 0
best_params = {}
for params in weights_to_test:
    precision = evaluate_hybrid_model(
        params['alpha'],
        params['beta'],
        params['gamma'],
        data=data,
        svd_model=svd,
        logreg_model=logreg_pipeline,
        nn_model=nn_model,
        user_id=500,
        top_n=10
    )
    print(f"Tested Params: {params}, Precision: {precision:.2f}")
    if precision > best_precision:
        best_precision = precision
        best_params = params

print(f"Best Parameters: {best_params}, Best Precision: {best_precision:.2f}")

# ----------------------------
# Example Usage
# ----------------------------
user_id = 500  # Change this to a valid user ID
top_recommendations = recommend_songs_hybrid_nn(
    user_id=user_id,
    data=data,
    svd_model=svd,
    logreg_model=logreg_pipeline,
    nn_model=nn_model,
    alpha=best_params.get('alpha', 0.5),
    beta=best_params.get('beta', 0.3),
    gamma=best_params.get('gamma', 0.2),
    top_n=10
)

# Display recommendations
print("\nTop 10 Recommended Songs:")
print(top_recommendations)

from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Predict on test set
y_test_preds = logreg_pipeline.predict(X_test)
y_test_probs = logreg_pipeline.predict_proba(X_test)[:, 1]  # Probabilities for ROC Curve

# Compute Accuracy
accuracy = accuracy_score(y_test, y_test_preds)
print(f"\nAccuracy: {accuracy:.2f}")

# Print Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_test_preds))

# Compute Confusion Matrix
cm = confusion_matrix(y_test, y_test_preds)

# Display Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Liked', 'Liked'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

# Compute ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_test_probs)
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random chance line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""## After CLustering"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split as surprise_train_test_split
from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report




# ----------------------------
# Load and Preprocess Data
# ----------------------------
data = pd.read_csv('/content/drive/MyDrive/colab-datasets/Spotify/data.csv')

# Define the target variable: Convert 'popularity' into a binary label
data['like'] = (data['popularity'] > 50).astype(int)

# Drop non-numeric and non-useful features
drop_columns = ['id', 'name', 'artists', 'release_date', 'popularity']  # Include 'cluster' in drop list
X = data.drop(columns=drop_columns + ['like'])
y = data['like']

# Split into training and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# ----------------------------
# Content-Based Filtering (Logistic Regression)
# ----------------------------
# Define Pipeline
logreg_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=10)),  # Reduce dimensionality
    ('logreg', LogisticRegression(max_iter=10000, class_weight='balanced', random_state=42))
])

# Train Logistic Regression
logreg_pipeline.fit(X_train_resampled, y_train_resampled)

# Predict probabilities for all songs
logreg_preds = logreg_pipeline.predict_proba(X)[:, 1]

# ----------------------------
# Collaborative Filtering (Matrix Factorization - SVD)
# ----------------------------
# Create user-song interaction matrix
user_song_data = data[['id', 'popularity']].copy()
user_song_data['user_id'] = np.random.randint(1, 1001, size=len(user_song_data))  # Fake user IDs
user_song_data['interaction'] = (user_song_data['popularity'] > 50).astype(int)

# Prepare data for Surprise library
reader = Reader(rating_scale=(0, 1))
interaction_data = Dataset.load_from_df(user_song_data[['user_id', 'id', 'interaction']], reader)

# Train-test split
trainset, testset = surprise_train_test_split(interaction_data, test_size=0.2)

# Train SVD model
svd = SVD(n_factors=50, random_state=42)
svd.fit(trainset)

# Generate SVD predictions for all songs
cf_preds = np.array([svd.predict(uid, iid).est for uid, iid in zip(user_song_data['user_id'], user_song_data['id'])])

# ----------------------------
# Similarity-Based Filtering (Nearest Neighbors)
# ----------------------------
# Normalize features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Fit Nearest Neighbors model using brute force algorithm
nn_model = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute')
nn_model.fit(X_scaled)

def get_similar_songs_nn(song_id, data, nn_model, n_neighbors=10):
    """
    Find similar songs using Nearest Neighbors.
    Parameters:
    - song_id: ID of the song to find similar songs for.
    - data: Dataset containing song features.
    - nn_model: Trained Nearest Neighbors model.
    - n_neighbors: Number of similar songs to return.
    Returns:
    - DataFrame of similar songs.
    """
    # Get index of the song
    song_index = data.index[data['id'] == song_id].tolist()[0]

    # Get feature vector of the song
    song_vector = X_scaled[song_index].reshape(1, -1)

    # Find nearest neighbors (excluding itself)
    distances, indices = nn_model.kneighbors(song_vector, n_neighbors=n_neighbors+1)

    # Get similar songs
    similar_songs = data.iloc[indices[0][1:]][['id', 'name', 'artists']]

    return similar_songs

from sklearn.cluster import KMeans

# ----------------------------
# Clustering Songs (K-Means)
# ----------------------------
num_clusters = 10  # Adjust based on dataset size
kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
data['cluster'] = kmeans.fit_predict(X_scaled)

# ----------------------------
# Hybrid Recommendation with Clustering
# ----------------------------
def recommend_songs_hybrid_nn(user_id, data, svd_model, logreg_model, nn_model, alpha=0.5, beta=0.3, gamma=0.2, delta=0.2, top_n=10):
    # Get songs the user has interacted with
    user_songs = user_song_data[user_song_data['user_id'] == user_id]

    if user_songs.empty:
        print("No user history found, recommending popular songs.")
        return data.sort_values(by="popularity", ascending=False).head(top_n)[['id', 'name', 'artists']]

    # Identify the most frequent clusters the user interacted with
    user_clusters = data[data['id'].isin(user_songs['id'])]['cluster'].value_counts().index[:3]

    # Get content-based predictions
    song_features = data.drop(columns=['id', 'name', 'artists', 'release_date', 'popularity', 'like', 'cluster'])  # Ensure 'cluster' is excluded
    logreg_preds = logreg_model.predict_proba(song_features)[:, 1]


    # Get collaborative filtering predictions (SVD)
    cf_preds = np.array([svd_model.predict(user_id, song_id).est for song_id in data['id']])

    # Get similarity-based recommendations
    similar_scores = np.zeros(len(data))
    for song_id in user_songs['id']:
        similar_songs = get_similar_songs_nn(song_id, data, nn_model, n_neighbors=10)
        similar_scores[data.index[data['id'].isin(similar_songs['id'])]] += 1
    similar_scores /= similar_scores.max()

    # Cluster-based scoring: Boost scores for songs in user-preferred clusters
    cluster_scores = data['cluster'].apply(lambda x: 1 if x in user_clusters else 0)

    # Normalize all scores
    scaler = MinMaxScaler()
    logreg_preds_norm = scaler.fit_transform(logreg_preds.reshape(-1, 1)).flatten()
    cf_preds_norm = scaler.fit_transform(cf_preds.reshape(-1, 1)).flatten()
    similar_scores_norm = scaler.fit_transform(similar_scores.reshape(-1, 1)).flatten()
    cluster_scores_norm = scaler.fit_transform(cluster_scores.values.reshape(-1, 1)).flatten()

    # Compute final hybrid score with clustering factor
    hybrid_scores = (
        alpha * logreg_preds_norm +
        beta * cf_preds_norm +
        gamma * similar_scores_norm +
        delta * cluster_scores_norm
    )

    recommendations = data[['id', 'name', 'artists']].copy()
    recommendations['hybrid_score'] = hybrid_scores
    recommendations = recommendations.sort_values(by='hybrid_score', ascending=False)

    return recommendations.head(top_n)


# ----------------------------
# Evaluate Hybrid Model
# ----------------------------
def evaluate_hybrid_model(alpha, beta, gamma, data, svd_model, logreg_model, nn_model, user_id, top_n=10):
    # Generate hybrid recommendations
    recommendations = recommend_songs_hybrid_nn(
        user_id=user_id,
        data=data,
        svd_model=svd_model,
        logreg_model=logreg_model,
        nn_model=nn_model,
        alpha=alpha,
        beta=beta,
        gamma=gamma,
        top_n=top_n
    )

    # Simulate ground truth relevance (e.g., based on popularity or other criteria)
    relevant_tracks = data.sort_values(by="popularity", ascending=False).head(top_n)['id'].values
    recommended_tracks = recommendations['id'].values

    # Debug: Print relevant and recommended tracks
    print(f"Relevant Tracks: {relevant_tracks}")
    print(f"Recommended Tracks: {recommended_tracks}")

    # Calculate precision@K
    precision = len(set(recommended_tracks).intersection(set(relevant_tracks))) / top_n if top_n > 0 else 0
    return precision

# ----------------------------
# Simplified Weight Tuning
# ----------------------------
weights_to_test = [
    {'alpha': 0.5, 'beta': 0.3, 'gamma': 0.2},
    {'alpha': 0.7, 'beta': 0.2, 'gamma': 0.1},
    {'alpha': 0.3, 'beta': 0.5, 'gamma': 0.2}
]

best_precision = 0
best_params = {}
for params in weights_to_test:
    precision = evaluate_hybrid_model(
        params['alpha'],
        params['beta'],
        params['gamma'],
        data=data,
        svd_model=svd,
        logreg_model=logreg_pipeline,
        nn_model=nn_model,
        user_id=500,
        top_n=10
    )
    print(f"Tested Params: {params}, Precision: {precision:.2f}")
    if precision > best_precision:
        best_precision = precision
        best_params = params

print(f"Best Parameters: {best_params}, Best Precision: {best_precision:.2f}")

# ----------------------------
# Example Usage
# ----------------------------
user_id = 500  # Change this to a valid user ID
top_recommendations = recommend_songs_hybrid_nn(
    user_id=user_id,
    data=data,
    svd_model=svd,
    logreg_model=logreg_pipeline,
    nn_model=nn_model,
    alpha=best_params.get('alpha', 0.5),
    beta=best_params.get('beta', 0.3),
    gamma=best_params.get('gamma', 0.2),
    top_n=10
)

# Display recommendations
print("\nTop 10 Recommended Songs:")
print(top_recommendations)

#  add clustering here

# ----------------------------
# Evaluate Classification Model (Logistic Regression)
# ----------------------------
# Predict on test set
y_test_pred = logreg_pipeline.predict(X_test)
y_test_prob = logreg_pipeline.predict_proba(X_test)[:, 1]

# Compute ROC-AUC Score
roc_auc = roc_auc_score(y_test, y_test_prob)
print(f"ROC-AUC Score: {roc_auc:.2f}")

# Generate Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

# Visualizing the Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Liked', 'Liked'], yticklabels=['Not Liked', 'Liked'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Classification Report
class_report = classification_report(y_test, y_test_pred)
print("\nClassification Report:")
print(class_report)

"""## NCF"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------
# Fix User-Item Interaction Data
# ----------------------------
if 'user_id' not in data.columns:
    np.random.seed(42)
    data['user_id'] = np.random.randint(1, 1001, size=len(data))  # Generate fake user IDs

if 'like' not in data.columns:
    data['like'] = (data['popularity'] > 50).astype(int)  # Define target

# Map users and items to unique indices
user2idx = {u: i for i, u in enumerate(data['user_id'].unique())}
item2idx = {i: j for j, i in enumerate(data['id'].unique())}

data['user_idx'] = data['user_id'].map(user2idx)
data['item_idx'] = data['id'].map(item2idx)

# ----------------------------
# Prepare Training Data for NCF
# ----------------------------
X_train = [data['user_idx'].values, data['item_idx'].values]
y_train = data['like'].values

# ----------------------------
# Define NCF Model
# ----------------------------
num_users = len(user2idx)
num_items = len(item2idx)
embedding_dim = 50

# Input Layers
user_input = Input(shape=(1,))
item_input = Input(shape=(1,))

# Embedding Layers
user_embedding = Embedding(num_users, embedding_dim)(user_input)
item_embedding = Embedding(num_items, embedding_dim)(item_input)

# Flatten Layers
user_vector = Flatten()(user_embedding)
item_vector = Flatten()(item_embedding)

# Concatenate User & Item Embeddings
concat = Concatenate()([user_vector, item_vector])

# Fully Connected Layers
hidden = Dense(128, activation='relu')(concat)
hidden = Dense(64, activation='relu')(hidden)
hidden = Dense(32, activation='relu')(hidden)
output = Dense(1, activation='sigmoid')(hidden)

# Compile Model
ncf_model = keras.Model(inputs=[user_input, item_input], outputs=output)
ncf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
ncf_model.summary()

# Train Model
ncf_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.3)

# ----------------------------
# Model Evaluation
# ----------------------------
# Get Predictions
pred_probs = ncf_model.predict(X_train).flatten()
pred_labels = (pred_probs >= 0.5).astype(int)

# Compute AUC-ROC
fpr, tpr, _ = roc_curve(y_train, pred_probs)
roc_auc = auc(fpr, tpr)

# Plot AUC-ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='darkorange', lw=2)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', lw=2)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("NCF Model - ROC Curve")
plt.legend(loc='lower right')
plt.show()

# Confusion Matrix
cm = confusion_matrix(y_train, pred_labels)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("NCF Model - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Classification Report
print("Classification Report:")
print(classification_report(y_train, pred_labels))

"""## Recommendation System"""

ngrok.kill()

"""# NCF + HYBRID"""

import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate
from tensorflow.keras.models import Model
from pyngrok import ngrok



# Assuming 'data' is loaded as a DataFrame (replace with actual dataset loading)

# Define target variable
data['like'] = (data['popularity'] > 50).astype(int)

# Drop non-numeric and non-useful features
drop_columns = ['id', 'name', 'artists', 'release_date', 'popularity']
X = data.drop(columns=drop_columns + ['like'])
y = data['like']

# Train-test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Train Content-Based Filtering (Logistic Regression)
logreg_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=10)),
    ('logreg', LogisticRegression(max_iter=10000, class_weight='balanced', random_state=42))
])
logreg_pipeline.fit(X_train_resampled, y_train_resampled)

def recommend_songs_ncf(user_id, data, model, top_n=10):
    if user_id not in user2idx:
        print("User not found! Recommending popular songs.")
        return data.sort_values(by="popularity", ascending=False).head(top_n)[['id', 'name', 'artists']]

    user_idx = user2idx[user_id]
    song_indices = np.array(list(item2idx.values()))
    user_indices = np.full_like(song_indices, user_idx)

    # Predict scores
    scores = model.predict([user_indices, song_indices]).flatten()

    # Get top-N recommendations
    top_song_indices = np.argsort(scores)[-top_n:][::-1]
    recommended_song_ids = [list(item2idx.keys())[i] for i in top_song_indices]

    return data[data['id'].isin(recommended_song_ids)][['id', 'name', 'artists']]

# Example Recommendation
user_id = 500
top_recommendations = recommend_songs_ncf(user_id, data, ncf_model, top_n=10)
print("\nTop 10 Recommended Songs:")
print(top_recommendations)

scaler_nn = MinMaxScaler()
X_scaled = scaler_nn.fit_transform(X)
nn_model = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute')
nn_model.fit(X_scaled)

def recommend_songs_hybrid_nn(user_id, data, ncf_model, logreg_model, nn_model,
                              danceability, energy, tempo, alpha=0.5, beta=0.3, gamma=0.2, top_n=10):
    """
    Recommend songs using a hybrid model that includes NCF, content-based, and similarity-based filtering.
    """
    # Content-based predictions using logistic regression
    song_features = data.drop(columns=['id', 'name', 'artists', 'release_date', 'popularity', 'like'])
    logreg_preds = logreg_model.predict_proba(song_features)[:, 1]

    # NCF predictions
    user_idx = user2idx.get(user_id, 0)
    song_indices = np.array(list(item2idx.values()))
    user_indices = np.full_like(song_indices, user_idx)
    ncf_preds = ncf_model.predict([user_indices, song_indices]).flatten()

    # Normalize scores
    norm_scaler = MinMaxScaler()
    logreg_preds_norm = norm_scaler.fit_transform(logreg_preds.reshape(-1, 1)).flatten()
    ncf_preds_norm = norm_scaler.fit_transform(ncf_preds.reshape(-1, 1)).flatten()

    # Compute hybrid score
    hybrid_scores = alpha * logreg_preds_norm + beta * ncf_preds_norm

    # Final recommendations
    recommendations = data[['id', 'name', 'artists']].copy()
    recommendations['final_score'] = hybrid_scores
    recommendations = recommendations.sort_values(by='final_score', ascending=False)

    return recommendations.head(top_n)

ngrok.kill()

!pip install dash plotly pyngrok

print("Years Available:", data['year'].unique())

# Import necessary libraries
import dash
from dash import dcc, html
from dash.dependencies import Input, Output, State
import plotly.express as px
from pyngrok import ngrok

# ----------------------------
# Initialize Dash App
# ----------------------------
app = dash.Dash(__name__)

# ----------------------------
# Dash App Layout
# ----------------------------
app.layout = html.Div([
    html.Div([
        html.H1("Spotify Recommendation Dashboard", style={'text-align': 'center', 'color': '#4CAF50'}),
        html.H2("Explore Tracks by Features", style={'text-align': 'center', 'color': '#555'}),
    ], style={'background-color': '#f0f0f0', 'padding': '20px'}),

    html.Div([
        html.Label("Select a Feature to Visualize:", style={'font-weight': 'bold'}),
        dcc.Dropdown(
            id='feature-dropdown',
            options=[{'label': col.capitalize(), 'value': col} for col in ['danceability', 'energy', 'valence', 'tempo', 'acousticness']],
            value='danceability',
            placeholder="Choose a feature",
            style={'width': '50%', 'margin-bottom': '20px'}
        ),
        html.Label("Select a Year:", style={'font-weight': 'bold'}),
        dcc.Dropdown(
            id='year-dropdown',
            options=[{'label': str(year), 'value': year} for year in sorted(data['year'].unique())],
            placeholder="Choose a year",
            style={'width': '50%'}
        )
    ], style={'text-align': 'center', 'padding': '20px'}),

    html.Div([dcc.Graph(id='feature-chart')], style={'margin-top': '20px', 'padding': '20px'}),
    html.Div([dcc.Graph(id='avg-duration-chart')], style={'margin-top': '20px', 'padding': '20px'}),
    html.Div([dcc.Graph(id='top-artists-chart')], style={'margin-top': '20px', 'padding': '20px'}),
    html.Div([dcc.Graph(id='cluster-chart')], style={'margin-top': '20px', 'padding': '20px'}),


    # Sliders for Recommendations
    html.Div([

        html.Div([
        html.H1("Spotify Recommendation Dashboard"),
        dcc.Slider(id='danceability-slider', min=0, max=1, step=0.1, value=0.5),
        dcc.Slider(id='energy-slider', min=0, max=1, step=0.11, value=0.5),
        dcc.Slider(id='tempo-slider', min=50, max=200, step=10, value=120),
        html.Button('Get Recommendations', id='submit-button'),
        html.Div(id='output-recommendations')
        ]),
    ], style={'padding': '20px'}),

    html.Div([
        html.P("Created with Dash and Plotly", style={'text-align': 'center', 'color': '#aaa'}),
    ], style={'background-color': '#f0f0f0', 'padding': '10px'}),


])

# ----------------------------
# Dash Callback to Update Charts
# ----------------------------
@app.callback(
    [Output('feature-chart', 'figure'),
     Output('avg-duration-chart', 'figure'),
     Output('top-artists-chart', 'figure'),
     Output('cluster-chart', 'figure')],
    [Input('feature-dropdown', 'value'),
     Input('year-dropdown', 'value')]
)
def update_charts(selected_feature, selected_year):
    # Feature Distribution Chart
    if selected_feature and selected_year:
        filtered_data = data[data['year'] == selected_year]
        feature_fig = px.histogram(
            filtered_data, x=selected_feature, nbins=20,
            title=f"Distribution of {selected_feature.capitalize()} in {selected_year}",
            labels={selected_feature: selected_feature.capitalize()},
            color_discrete_sequence=['#636EFA']
        )
    else:
        feature_fig = px.histogram(
            data, x=selected_feature, nbins=20,
            title=f"Overall Distribution of {selected_feature.capitalize()}",
            labels={selected_feature: selected_feature.capitalize()},
            color_discrete_sequence=['#636EFA']
        )

    # Average Duration Chart
    avg_duration_by_year = data.groupby('year')['duration_ms'].mean().reset_index()
    duration_fig = px.line(
        avg_duration_by_year, x='year', y='duration_ms',
        title="Average Duration (ms) by Year",
        labels={'duration_ms': 'Average Duration (ms)', 'year': 'Year'},
        line_shape='linear',
        color_discrete_sequence=['#FF6347']
    )
    # Top 10 Artists by Average Duration Chart
    avg_duration_by_artist = data.groupby('artists')['duration_ms'].mean().reset_index()
    top_10_artists = avg_duration_by_artist.nlargest(10, 'duration_ms')

    artist_fig = px.bar(
        top_10_artists, x='artists', y='duration_ms',
        title="Top 10 Artists by Average Duration (ms)",
        labels={'artists': 'Artists', 'duration_ms': 'Average Duration (ms)'},
        color='duration_ms',
        color_continuous_scale='Blues'
    )

    # Cluster Visualization
    cluster_fig = px.scatter(
        data, x='tempo', y='energy',
        color='cluster',
        title="Clusters of Songs",
        labels={'tempo': 'Tempo', 'energy': 'Energy', 'cluster': 'Cluster'},
        color_continuous_scale='viridis'
    )

    return feature_fig, duration_fig, artist_fig, cluster_fig

@app.callback(
    Output('output-recommendations', 'children'),
    Input('submit-button', 'n_clicks'),
    [Input('danceability-slider', 'value'),
     Input('energy-slider', 'value'),
     Input('tempo-slider', 'value')]
)
def update_recommendations(n_clicks, danceability, energy, tempo):
    user_id = 500  # Example user ID
    recommendations = recommend_songs_hybrid_nn(user_id, data, ncf_model, logreg_pipeline, nn_model, danceability, energy, tempo)
    return html.Table([
        html.Tr([html.Th(col) for col in recommendations.columns])] +
        [html.Tr([html.Td(recommendations.iloc[i][col]) for col in recommendations.columns]) for i in range(len(recommendations))]
    )

# ----------------------------
# Ngrok Setup & Run Server
# ----------------------------
ngrok.kill()
ngrok.set_auth_token("2sXZmjJ7x0txWk3iY6hE0aytq3v_22hocpmMYEVCNNkSuyViC")
public_url = ngrok.connect(8050)
print("Dash app is running at:", public_url)

# Run the app
if __name__ == '__main__':
    app.run_server(debug=False)

